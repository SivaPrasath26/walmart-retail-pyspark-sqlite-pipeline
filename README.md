#  Retail PySpark SQLite Pipeline

##  Overview
This project demonstrates an **enterprise-grade PySpark data pipeline** that ingests the **Retail Data Analytics (Walmart)** dataset from Kaggle, performs data quality checks, transformations, and outputs a curated dataset into a **local SQLite database** â€” ready for **Power BI** or other BI tools.

It is designed with **modular architecture**, **YAML-based configuration**, and **production best practices**.

---

##  Folder Structure

project_root/
â”‚
â”œâ”€â”€ config/
â”‚ â”œâ”€â”€ pipeline_config.yaml # All pipeline parameters, schema, paths, renaming rules
â”‚
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ raw/ # Unprocessed datasets from Kaggle
â”‚ â”œâ”€â”€ curated/ # Final SQLite DB output
â”‚
â”œâ”€â”€ logs/
â”‚ â”œâ”€â”€ pipeline.log # Pipeline logs
â”‚
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ init.py
â”‚ â”œâ”€â”€ ingestion_kaggle.py # Kaggle download & extraction
â”‚ â”œâ”€â”€ quality_checks.py # Schema, nulls, duplicates
â”‚ â”œâ”€â”€ transform_spark.py # PySpark transformations & enrichment
â”‚ â”œâ”€â”€ pipeline_orchestrator.py # Orchestrates the pipeline
â”‚
â”œâ”€â”€ requirements.txt # Python dependencies
â”œâ”€â”€ README.md # Project documentation
â””â”€â”€ .env # Kaggle API credentials

## Setup Instructions

### 1ï¸. Clone Repo
```bash
git clone https://github.com/YOUR_USERNAME/retail-pyspark-sqlite-pipeline.git
cd retail-pyspark-sqlite-pipeline
```

### 2ï¸. Create & Activate Virtual Environment
```bash
python -m venv venv
source venv/bin/activate     # Mac/Linux
venv\Scripts\activate        # Windows
```

### 3ï¸. Install Dependencies
```bash
pip install -r requirements.txt
```

### 4ï¸. Kaggle API Setup
- Sign up / log in to [Kaggle](https://www.kaggle.com/).
- Generate API Token from **Account Settings** â†’ **Create New API Token**.
- Save `kaggle.json` to `~/.kaggle/kaggle.json`  
  **OR** create a `.env` file in the project root:
```
KAGGLE_USERNAME=your_username
KAGGLE_KEY=your_key
```

### 5ï¸. Configure Pipeline
Update `config/pipeline_config.yaml`:
- File paths  
- Column rename mappings  
- Schema definitions  
- Output SQLite table names  

---

## 6. Run Pipeline
```bash
python src/pipeline_orchestrator.py
```

**Flow**:
1. **Ingestion** â†’ Downloads Walmart dataset from Kaggle into `data/raw/`
2. **Quality Checks** â†’ Validates schema, nulls, duplicates
3. **Transformation** â†’ Merges datasets, renames columns, enriches data
4. **SQLite Output** â†’ Saves final tables into `data/curated/retail_pipeline.db`

---

## Power BI Integration
1. Open **Power BI Desktop**
2. Select **Get Data** â†’ **SQLite Database**
3. Connect to `data/curated/retail_pipeline.db`
4. Build dashboards using curated tables.

---

## Key Features
- **PySpark-based** transformations for scalability
- **YAML config** for flexible parameters
- **Logging** to `logs/pipeline.log`
- **SQLite** output for BI-friendly analytics
- **Modular, production-grade design**

---

## ğŸ— Tech Stack
- Python 3.10+
- PySpark
- Pandas
- SQLite
- YAML
- Logging
